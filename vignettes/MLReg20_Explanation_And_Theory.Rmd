---
title: "MLReg20_Explanation_And_Theory"
output: rmarkdown::html_vignette
bibliography: proj3.bib
vignette: >
  %\VignetteIndexEntry{MLReg20_Explanation_And_Theory}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(MLReg20)
```


MLReg20 is a package to help assess the validity and usefulness of a multiple linear regression model. A multiple linear regression model has the form $y=\beta_0 + \beta_1x_1+\beta_2x_2+...+\beta_kx_k+\epsilon$ where the terms of the form $\beta_ix_i$ are in the deterministic portion of the model and $\epsilon$ is the error term of the model. The model can be written in matrix form as $Y=X\beta+\epsilon$ where $Y$ is the $n \times 1$ vector of all the responses, $X$ is the $n \times k+1$ design matrix with the first column filled with 1's and all other columns filled with the predictor variables, and $\beta$ is the $k+1 \times 1$ vector of regression coefficients. We can use the observed values to create $\hat{\beta}$, the estimates for the $\beta$'s. Then we can use $Y=\hat{\beta}X$ as a predictive model given different values of the predictor variables. Linear models make assumptions about the $\epsilon$. One large assumption is that the error term is normally distributed around 0. That is $\epsilon \sim N(0, \sigma^2I)$ where $I$ is the identity matrix. When the error is normal, then the expected value $E(y)=E(X\hat{\beta}+\epsilon)=XE(\hat{\beta})+E(\epsilon)=X\beta+0=X\beta$. Therefore, the normality of $\epsilon$ allows us to use the model for predictions. The model's utility as a predictive model depends on the assumptions being met, and it also depends on the confidence of the $\beta$ estimates. The functions valCheck and mlrBoot perform analyses of the assumptions and the confidence of the estimates respectively. The function bayesMLR creates a model based on the bayesian paradigm and compares it to the model from the classical paradigm. Finally, mlrInteractive produces a shiney server to interact with plots. 

The first function, valCheck, examines the validity of a linear model. It checks the main assumptions of a linear model. The central assumption of a linear regression model is that the residuals are independent identically distributed normal (iid normal). That can be broken down into three clear assumptions: residuals are independent, residuals are homoscedastic, and residuals follow a normal distribution. Another important assumption of the multiple linear regression is that there is no multicollinearity between the independent variables. The function will produce descriptive statistics and plots that check each of the four assumptions. Additionally, since multiple linear regression models are sometimes sensitive to outliers, valCheck will produce a plot to indicate outliers. 

For the residuals to be independent, they must not be significantly autocorrelated. Autocorrelation occurs when a residual is affected by the previous residual. This assumption is very important to check in a time series regression or any regression where the samples are ordered. This assumption is checked if first order models with the Durbin-Watson test [@autocorr]. The Durbin-Watson test returns a number between 0 and 4, with 2 indicating no autocorrelation. Values between 1.5 and 2.5 indicate that the model is sufficiently non-autocorrelated. The function valCheck will provide the Durbin-Watson number if the parameter series=True is set. Otherwise it will be assumed that the assumption on independence is satisfied by design. 

For the residuals to be homoscedastic, their variance must be constant across all input values. A great way to visualize the variance of the residuals is plotting the residuals against the fitted values. If the variance is constant, we would expect to see random noise centered around 0. The sum of the residuals should be approximately 0, and there should be no clear signal within the residuals. The Breusch-Pagan method tests the assumption of homoscedasticity [@bp]. The null hypothesis is that the variance is constant, so a insignificant p-value indicates insufficient evidence to reject homoscedasticity. The function valCheck will provide a residual vs. fitted value plot as well as the Breusch-Pagan BP value and associated p-value. 

The residuals must follow a normal distribution. Other distributions would indicate that the model is not accounting for some significant structure in the data. A distribution can be visualized with a density plot. A density plot showing normally distributed residuals would appear bell shaped. Two distributions can also be compared using a QQ plot [@QQ]. A QQ plot compares the quantiles from two different distributions. If the quantiles being compared come from the same distribution, a straight line is expected. Therefore the sample quantiles can be plotted against the quantiles from a theoretical normal distribution. A third way to assess normality is a Shapiro-Wilks test. The null hypothesis of the Shapiro-Wilks test is normality, so an insignificant p-value indicated insufficient evidence to reject normality. The function valCheck will produce two plots to visually assess normality. The first plot will be a histogram with a smoothed curve over it. The curve can be compared to the normal bell shaped curve. The second plot will be a QQ plot of the quantiles of the samples against the theoretical quantiles of a normal distribution. valCheck will also print the Shapiro-Wilks value as well as the associated p-value. 

Multicollinearity refers to the linear relationships between independent (predictor) variables that do not relate the the linear relationship that each predictor shares with the dependent (response) variable. If an independent variable is continuous, then it should vary linearly with the dependent variable. Then two independent variables that both vary linearly with the dependent variable will vary linearly with each other. But if the shared relationship with the dependent variable is accounted for, then the relationship between the two independent variables should be random. If there is a linear relationship left over, they are considered multicollinear, and the assumption of the linear model is violated. A way to check for multicollinearity is checking VIF scores [@jim]. A VIF score will give a score to each variable to indicate how correlated it is to any other variable in the model. VIF scores greater than 10 in a first order model indicate an issue. VIF scores can be inflated if the model has interaction. The function valCheck will produce the VIF scores. 

Finally, outliers can throw off a multiple linear regression model. Cook's distance is a measure of how influential a particular sample is. If removing the sample has an effect on the model, the Cook's distance will be high [@cook]. A sample with a Cook's distance greater than 1 should almost always be removed, and a sample with a Cook's distance greater than 0.5 should be looked at very carefully and removal may be suggested. The function valCheck will produce a plot that shows the Cook's distance for each sample and has the samples with Cook's distances greater than 0.5 indicated. 

The second function, mlrBoot, runs a bootstrap analysis to estimate the beta values and produce confidence intervals for the values. A bootstrap analysis takes a sampling (with replacement) of all of the rows of the design matrix to create a new design matrix. Then the least squares estimates for all of the betas are preformed. The process is iterated a set number of times, designated by the iter parameter of the function. Estimates and confidence intervals for the bootstrapped betas are shown next to estimates and confidence intervals for the betas from the non-bootstrapped data set. Additionally, the function will return the number of times the bootstrap analysis failed because a singular matrix was produced by the bootstrapping process. Histograms for each beta are created that show the distribution of all of the estimates created by the bootstrap process. Since the betas should follow a normal distribution, the estimate printed to the command line is the mean and the confidence intervals are the $1-\alpha/2$ to $\alpha/2$ quantiles calculated from the distribution. The $\alpha$ is set at 0.05 default, but it can be changed. 

